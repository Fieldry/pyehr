{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess the CDSL Dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "import datetime\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "\n",
    "from datasets.preprocess.tools import forward_fill_pipeline, normalize_dataframe\n",
    "\n",
    "data_dir = \"./datasets/cdsl/\"\n",
    "Path(os.path.join(data_dir, 'processed')).mkdir(parents=True, exist_ok=True)\n",
    "Path(os.path.join(data_dir, 'statistics')).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "SEED = 42"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Demographic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demographic = pd.read_csv(os.path.join(data_dir, 'raw', '19_04_2021/COVID_DSL_01.CSV'), encoding='ISO-8859-1', sep='|')\n",
    "print(len(demographic))\n",
    "demographic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "med = pd.read_csv(os.path.join(data_dir, 'raw', '19_04_2021/COVID_DSL_04.CSV'), encoding='ISO-8859-1', sep='|')\n",
    "print(len(med))\n",
    "med.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(med['ID_ATC7'].unique())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exclude patients with missing labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(demographic))\n",
    "demographic = demographic.dropna(axis=0, how='any', subset=['IDINGRESO', 'F_INGRESO_ING', 'F_ALTA_ING', 'MOTIVO_ALTA_ING'])\n",
    "print(len(demographic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outcome2num(x):\n",
    "    if x == 'Fallecimiento':\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def to_one_hot(x, feature):\n",
    "    if x == feature:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select necessary columns from demographic\n",
    "demographic = demographic[\n",
    "        [\n",
    "            'IDINGRESO', \n",
    "            'EDAD',\n",
    "            'SEX',\n",
    "            'F_INGRESO_ING', \n",
    "            'F_ALTA_ING', \n",
    "            'MOTIVO_ALTA_ING', \n",
    "            'ESPECIALIDAD_URGENCIA', \n",
    "            'DIAG_URG'\n",
    "        ]\n",
    "    ]\n",
    "\n",
    "# rename column\n",
    "demographic = demographic.rename(columns={\n",
    "    'IDINGRESO': 'PATIENT_ID',\n",
    "    'EDAD': 'AGE',\n",
    "    'SEX': 'SEX',\n",
    "    'F_INGRESO_ING': 'ADMISSION_DATE',\n",
    "    'F_ALTA_ING': 'DEPARTURE_DATE',\n",
    "    'MOTIVO_ALTA_ING': 'OUTCOME',\n",
    "    'ESPECIALIDAD_URGENCIA': 'DEPARTMENT_OF_EMERGENCY',\n",
    "    'DIAG_URG': 'DIAGNOSIS_AT_EMERGENCY_VISIT'\n",
    "})\n",
    "\n",
    "# SEX: Male: 1; Female: 0\n",
    "demographic['SEX'].replace('MALE', 1, inplace=True)\n",
    "demographic['SEX'].replace('FEMALE', 0, inplace=True)\n",
    "\n",
    "# outcome: Fallecimiento(dead): 1; others: 0\n",
    "demographic['OUTCOME'] = demographic['OUTCOME'].map(outcome2num)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only reserve useful columns in demographic table\n",
    "demographic = demographic[\n",
    "        [\n",
    "            'PATIENT_ID',\n",
    "            'AGE',\n",
    "            'SEX',\n",
    "            'ADMISSION_DATE',\n",
    "            'DEPARTURE_DATE',\n",
    "            'OUTCOME',\n",
    "            # 'DIFFICULTY_BREATHING',\n",
    "            # 'SUSPECT_COVID',\n",
    "            # 'FEVER',\n",
    "            # 'EMERGENCY'\n",
    "        ]\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demographic.describe().to_csv(os.path.join(data_dir, 'statistics', 'demographic_overview.csv'), index=False)\n",
    "demographic.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demographic.to_csv(os.path.join(data_dir, 'processed', 'demographic.csv'), index=False)\n",
    "demographic.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Vital Signal Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vital_signs = pd.read_csv(os.path.join(data_dir, 'raw', '19_04_2021/COVID_DSL_02.CSV'), encoding='ISO-8859-1', sep='|')\n",
    "print(len(vital_signs))\n",
    "vital_signs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vital_signs = vital_signs.rename(columns={\n",
    "    'IDINGRESO': 'PATIENT_ID',\n",
    "    'CONSTANTS_ING_DATE': 'RECORD_DATE',\n",
    "    'CONSTANTS_ING_TIME': 'RECORD_TIME',\n",
    "    'FC_HR_ING': 'HEART_RATE',\n",
    "    'GLU_GLY_ING': 'BLOOD_GLUCOSE',\n",
    "    'SAT_02_ING': 'OXYGEN_SATURATION',\n",
    "    'TA_MAX_ING': 'MAX_BLOOD_PRESSURE',\n",
    "    'TA_MIN_ING': 'MIN_BLOOD_PRESSURE',\n",
    "    'TEMP_ING': 'TEMPERATURE'\n",
    "})\n",
    "vital_signs['RECORD_TIME'] = vital_signs['RECORD_DATE'] + ' ' + vital_signs['RECORD_TIME']\n",
    "vital_signs['RECORD_TIME'] = vital_signs['RECORD_TIME'].map(lambda x: str(datetime.datetime.strptime(x, '%Y-%m-%d %H:%M')))\n",
    "vital_signs = vital_signs.drop(['RECORD_DATE', 'SAT_02_ING_OBS', 'BLOOD_GLUCOSE'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vital_signs.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vital_signs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_temperature(x):\n",
    "    if type(x) == str:\n",
    "        return float(x.replace(',', '.'))\n",
    "    else:\n",
    "        return float(x)\n",
    "\n",
    "def format_oxygen(x):\n",
    "    x = float(x)\n",
    "    if x > 100:\n",
    "        return np.nan\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "def format_heart_rate(x):\n",
    "    x = int(x)\n",
    "    if x > 220:\n",
    "        return np.nan\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "vital_signs['TEMPERATURE'] = vital_signs['TEMPERATURE'].map(lambda x: format_temperature(x))\n",
    "vital_signs['OXYGEN_SATURATION'] = vital_signs['OXYGEN_SATURATION'].map(lambda x: format_oxygen(x))\n",
    "vital_signs['HEART_RATE'] = vital_signs['HEART_RATE'].map(lambda x: format_heart_rate(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vital_signs = vital_signs.replace(0, np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vital_signs = vital_signs.groupby(['PATIENT_ID', 'RECORD_TIME'], dropna=True, as_index = False).mean()\n",
    "vital_signs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vital_signs.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vital_signs.describe().to_csv(os.path.join(data_dir, 'statistics', 'vital_signs_overview.csv'), index=False)\n",
    "vital_signs.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vital_signs.to_csv(os.path.join(data_dir, 'processed', 'visual_signs.csv'), index=False)\n",
    "vital_signs.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Lab Tests Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab_tests = pd.read_csv(os.path.join(data_dir, 'raw', '19_04_2021/COVID_DSL_06_v2.CSV'), encoding='ISO-8859-1', sep=';')\n",
    "lab_tests = lab_tests.rename(columns={'IDINGRESO': 'PATIENT_ID'})\n",
    "print(len(lab_tests))\n",
    "\n",
    "# only reserve useful columns\n",
    "lab_tests = lab_tests[\n",
    "        [\n",
    "            'PATIENT_ID',\n",
    "            'LAB_NUMBER',\n",
    "            'LAB_DATE',\n",
    "            'TIME_LAB',\n",
    "            'ITEM_LAB',\n",
    "            'VAL_RESULT'\n",
    "            # UD_RESULT: unit\n",
    "            # REF_VALUES: reference values\n",
    "        ]\n",
    "    ]\n",
    "\n",
    "lab_tests.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab_tests = lab_tests.groupby(['PATIENT_ID', 'LAB_NUMBER', 'LAB_DATE', 'TIME_LAB', 'ITEM_LAB'], dropna=True, as_index = False).first()\n",
    "lab_tests = lab_tests.set_index(['PATIENT_ID', 'LAB_NUMBER', 'LAB_DATE', 'TIME_LAB', 'ITEM_LAB'], drop = True).unstack('ITEM_LAB')['VAL_RESULT'].reset_index()\n",
    "\n",
    "lab_tests = lab_tests.drop([\n",
    "    'CFLAG -- ALARMA HEMOGRAMA', \n",
    "    'CORONA -- PCR CORONAVIRUS 2019nCoV', \n",
    "    'CRIOGLO -- CRIOGLOBULINAS',\n",
    "    'EGCOVID -- ESTUDIO GENETICO COVID-19',\n",
    "    'FRO1 -- ',\n",
    "    'FRO1 -- FROTIS EN SANGRE PERIFERICA',\n",
    "    'FRO2 -- ',\n",
    "    'FRO2 -- FROTIS EN SANGRE PERIFERICA',\n",
    "    'FRO3 -- ',\n",
    "    'FRO3 -- FROTIS EN SANGRE PERIFERICA',\n",
    "    'FRO_COMEN -- ',\n",
    "    'FRO_COMEN -- FROTIS EN SANGRE PERIFERICA',\n",
    "    'G-CORONAV (RT-PCR) -- Tipo de muestra: ASPIRADO BRONCOALVEOLAR',\n",
    "    'G-CORONAV (RT-PCR) -- Tipo de muestra: EXUDADO',\n",
    "    'GRRH -- GRUPO SANGUÖNEO Y FACTOR Rh',\n",
    "    'HEML -- RECUENTO CELULAR LIQUIDO',\n",
    "    'HEML -- Recuento Hemat¡es',\n",
    "    'IFSUERO -- INMUNOFIJACION EN SUERO',\n",
    "    'OBS_BIOMOL -- OBSERVACIONES GENETICA MOLECULAR',\n",
    "    'OBS_BIOO -- Observaciones Bioqu¡mica Orina',\n",
    "    'OBS_CB -- Observaciones Coagulaci¢n',\n",
    "    'OBS_GASES -- Observaciones Gasometr¡a Arterial',\n",
    "    'OBS_GASV -- Observaciones Gasometr¡a Venosa',\n",
    "    'OBS_GEN2 -- OBSERVACIONES GENETICA',\n",
    "    'OBS_HOR -- Observaciones Hormonas',\n",
    "    'OBS_MICRO -- Observaciones Microbiolog¡a',\n",
    "    'OBS_NULA2 -- Observaciones Bioqu¡mica',\n",
    "    'OBS_NULA3 -- Observaciones Hematolog¡a',\n",
    "    'OBS_PESP -- Observaciones Pruebas especiales',\n",
    "    'OBS_SERO -- Observaciones Serolog¡a',\n",
    "    'OBS_SIS -- Observaciones Orina',\n",
    "    'PCR VIRUS RESPIRATORIOS -- Tipo de muestra: ASPIRADO BRONCOALVEOLAR',\n",
    "    'PCR VIRUS RESPIRATORIOS -- Tipo de muestra: BAS',\n",
    "    'PCR VIRUS RESPIRATORIOS -- Tipo de muestra: ESPUTO',\n",
    "    'PCR VIRUS RESPIRATORIOS -- Tipo de muestra: EXUDADO',\n",
    "    'PCR VIRUS RESPIRATORIOS -- Tipo de muestra: LAVADO BRONCOALVEOLAR',\n",
    "    'PCR VIRUS RESPIRATORIOS -- Tipo de muestra: LAVADO NASOFARÖNGEO',\n",
    "    'PTGOR -- PROTEINOGRAMA ORINA',\n",
    "    'RESUL_IFT -- ESTUDIO DE INMUNOFENOTIPO',\n",
    "    'RESUL_IFT -- Resultado',\n",
    "    'Resultado -- Resultado',\n",
    "    'SED1 -- ',\n",
    "    'SED1 -- SEDIMENTO',\n",
    "    'SED2 -- ',\n",
    "    'SED2 -- SEDIMENTO',\n",
    "    'SED3 -- ',\n",
    "    'SED3 -- SEDIMENTO',\n",
    "    'TIPOL -- TIPO DE LIQUIDO',\n",
    "    'Tecnica -- T\\x82cnica',\n",
    "    'TpMues -- Tipo de muestra',\n",
    "    'VHCBLOT -- INMUNOBLOT VIRUS HEPATITIS C',\n",
    "    'VIR_TM -- VIRUS TIPO DE MUESTRA',\n",
    "    'LEGIORI -- AG. LEGIONELA PNEUMOPHILA EN ORINA',\n",
    "    'NEUMOORI -- AG NEUMOCOCO EN ORINA',\n",
    "    'VIHAC -- VIH AC'\n",
    "    ], axis=1)\n",
    "\n",
    "\n",
    "lab_tests.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab_tests = lab_tests.replace('Sin resultado.', np.nan)\n",
    "lab_tests = lab_tests.replace('Sin resultado', np.nan)\n",
    "lab_tests = lab_tests.replace('----', np.nan).replace('---', np.nan)\n",
    "lab_tests = lab_tests.replace('> ', '').replace('< ', '')\n",
    "\n",
    "def change_format(x):\n",
    "    if x is None:\n",
    "        return np.nan\n",
    "    elif type(x) == str:\n",
    "        if x.startswith('Negativo ('):\n",
    "            return x.replace('Negativo (', '-')[:-1]\n",
    "        elif x.startswith('Positivo ('):\n",
    "            return x.replace('Positivo (', '')[:-1]\n",
    "        elif x.startswith('Zona limite ('):\n",
    "            return x.replace('Zona limite (', '')[:-1]\n",
    "        elif x.startswith('>'):\n",
    "            return x.replace('> ', '').replace('>', '')\n",
    "        elif x.startswith('<'):\n",
    "            return x.replace('< ', '').replace('<', '')\n",
    "        elif x.endswith(' mg/dl'):\n",
    "            return x.replace(' mg/dl', '')\n",
    "        elif x.endswith('/æl'):\n",
    "            return x.replace('/æl', '')\n",
    "        elif x.endswith(' copias/mL'):\n",
    "            return x.replace(' copias/mL', '')\n",
    "        elif x == 'Numerosos':\n",
    "            return 1.5\n",
    "        elif x == 'Aislados':\n",
    "            return 0.5\n",
    "        elif x == 'Se detecta' or x == 'Se observan' or x == 'Normal' or x == 'Positivo':\n",
    "            return 1\n",
    "        elif x == 'No se detecta' or x == 'No se observan' or x == 'Negativo':\n",
    "            return 0\n",
    "        elif x == 'Indeterminado':\n",
    "            return np.nan\n",
    "        else:\n",
    "            num = re.findall(\"[-+]?\\d+\\.\\d+\", x)\n",
    "            if len(num) == 0:\n",
    "                return np.nan\n",
    "            else:\n",
    "                return num[0]\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "feature_value_dict = dict()\n",
    "\n",
    "for k in tqdm(lab_tests.keys()[4:]):\n",
    "    lab_tests[k] = lab_tests[k].map(lambda x: change_format(change_format(x)))\n",
    "    feature_value_dict[k] = lab_tests[k].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nan_and_not_nan(x):\n",
    "    if x == x:\n",
    "        return 1\n",
    "    else: # nan\n",
    "        return 0\n",
    "\n",
    "def is_float(num):\n",
    "    try:\n",
    "        float(num)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "def is_all_float(x):\n",
    "    for i in x:\n",
    "        if i == i and (i != None):\n",
    "            if not is_float(i):\n",
    "                return False\n",
    "    return True\n",
    "\n",
    "def to_float(x):\n",
    "    if x != None:\n",
    "        return float(x)\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "other_feature_dict = dict()\n",
    "\n",
    "for feature in tqdm(feature_value_dict.keys()):\n",
    "    values = feature_value_dict[feature]\n",
    "    if is_all_float(values):\n",
    "        lab_tests[feature] = lab_tests[feature].map(lambda x: to_float(x))\n",
    "    elif len(values) == 2:\n",
    "        lab_tests[feature] = lab_tests[feature].map(lambda x: nan_and_not_nan(x))\n",
    "    else:\n",
    "        other_feature_dict[feature] = values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_time(t):\n",
    "    if '/' in t:\n",
    "        return str(datetime.datetime.strptime(t, '%d/%m/%Y %H:%M'))\n",
    "    else:\n",
    "        return str(datetime.datetime.strptime(t, '%d-%m-%Y %H:%M'))\n",
    "\n",
    "lab_tests['RECORD_TIME'] = lab_tests['LAB_DATE'] + ' ' + lab_tests['TIME_LAB']\n",
    "lab_tests['RECORD_TIME'] = lab_tests['RECORD_TIME'].map(lambda x: format_time(x))\n",
    "lab_tests = lab_tests.drop(['LAB_NUMBER', 'LAB_DATE', 'TIME_LAB'], axis=1)\n",
    "lab_tests.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab_tests_patient = lab_tests.groupby(['PATIENT_ID'], dropna=True, as_index = False).mean(numeric_only=True)\n",
    "print(len(lab_tests_patient))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_total = len(lab_tests_patient)\n",
    "threshold = patient_total * 0.1\n",
    "reserved_keys = []\n",
    "\n",
    "for key in lab_tests_patient.keys():\n",
    "    if lab_tests_patient[key].count() > threshold:\n",
    "        reserved_keys.append(key)\n",
    "\n",
    "print(len(reserved_keys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reserved_keys.insert(1, 'RECORD_TIME')\n",
    "lab_tests = lab_tests.groupby(['PATIENT_ID', 'RECORD_TIME'], dropna=True, as_index = False).mean()\n",
    "lab_tests = lab_tests[reserved_keys]\n",
    "lab_tests.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab_tests.to_csv(os.path.join(data_dir, 'processed', 'lab_test.csv'), index=False)\n",
    "lab_tests.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concat data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demographic['PATIENT_ID'] = demographic['PATIENT_ID'].map(lambda x: str(int(x)))\n",
    "vital_signs['PATIENT_ID'] = vital_signs['PATIENT_ID'].map(lambda x: str(int(x)))\n",
    "lab_tests['PATIENT_ID'] = lab_tests['PATIENT_ID'].map(lambda x: str(int(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(demographic['PATIENT_ID'].unique()), len(vital_signs['PATIENT_ID'].unique()), len(lab_tests['PATIENT_ID'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(vital_signs, lab_tests, on=['PATIENT_ID', 'RECORD_TIME'], how='outer')\n",
    "df = df.groupby(['PATIENT_ID', 'RECORD_TIME'], dropna=True, as_index = False).mean()\n",
    "df = pd.merge(demographic, df, on=['PATIENT_ID'], how='left')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del rows without patient_id, admission_date, record_time, or outcome\n",
    "df = df.dropna(axis=0, how='any', subset=['PATIENT_ID', 'ADMISSION_DATE', 'RECORD_TIME', 'OUTCOME'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(os.path.join(data_dir, 'processed', 'cdsl_dataset_all.csv'), index=False)\n",
    "df.describe()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export to unified CSV dataset format "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- features: demographic & lab test & vital signs\n",
    "- targets: outcome & length of stay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_ids = df['PATIENT_ID'].unique()\n",
    "\n",
    "demo_cols = ['AGE', 'SEX'] # , 'DIFFICULTY_BREATHING', 'FEVER', 'SUSPECT_COVID', 'EMERGENCY'\n",
    "test_cols = []\n",
    "\n",
    "# get column names\n",
    "for k in df.keys():\n",
    "    if not k in demographic.keys():\n",
    "        if not k == 'RECORD_TIME':\n",
    "            test_cols.append(k)\n",
    "\n",
    "test_median = df[test_cols].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['RECORD_TIME_DAY'] = df['RECORD_TIME'].map(lambda x: datetime.datetime.strptime(x, '%Y-%m-%d %H:%M:%S').strftime('%Y-%m-%d'))\n",
    "df['RECORD_TIME_HOUR'] = df['RECORD_TIME'].map(lambda x: datetime.datetime.strptime(x, '%Y-%m-%d %H:%M:%S').strftime('%Y-%m-%d %H'))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_day = df.groupby(['PATIENT_ID', 'ADMISSION_DATE', 'DEPARTURE_DATE', 'RECORD_TIME_DAY'], dropna=True, as_index = False).mean(numeric_only=True)\n",
    "df_hour = df.groupby(['PATIENT_ID', 'ADMISSION_DATE', 'DEPARTURE_DATE', 'RECORD_TIME_HOUR'], dropna=True, as_index = False).mean(numeric_only=True)\n",
    "\n",
    "len(df), len(df_day), len(df_hour)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of visits (total):\n",
    "\n",
    "- Original data: 168777\n",
    "- Merge by hour: 130141\n",
    "- Merge by day:  42204"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df['PATIENT_ID'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hour['LOS'] = df_hour['ADMISSION_DATE']\n",
    "df_hour['LOS_HOUR'] = df_hour['ADMISSION_DATE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hour = df_hour.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in tqdm(range(len(df_hour))):\n",
    "    info = df_hour.loc[idx]\n",
    "    admission = datetime.datetime.strptime(info['ADMISSION_DATE'], '%Y-%m-%d %H:%M:%S')\n",
    "    departure = datetime.datetime.strptime(info['DEPARTURE_DATE'], '%Y-%m-%d %H:%M:%S')\n",
    "    visit_hour = datetime.datetime.strptime(info['RECORD_TIME_HOUR'], '%Y-%m-%d %H')\n",
    "    hour = (departure - visit_hour).seconds / (24 * 60 * 60) + (departure - visit_hour).days\n",
    "    los = (departure - admission).seconds / (24 * 60 * 60) + (departure - admission).days\n",
    "    df_hour.at[idx, 'LOS'] = float(los)\n",
    "    df_hour.at[idx, 'LOS_HOUR'] = float(hour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hour_idx = df_hour.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hour_idx['LOS'] = df_hour_idx['ADMISSION_DATE']\n",
    "\n",
    "for idx in tqdm(range(len(df_hour_idx))):\n",
    "    info = df_hour_idx.loc[idx]\n",
    "    # admission = datetime.datetime.strptime(info['ADMISSION_DATE'], '%Y-%m-%d %H:%M:%S')\n",
    "    departure = datetime.datetime.strptime(info['DEPARTURE_DATE'], '%Y-%m-%d %H:%M:%S')\n",
    "    visit_hour = datetime.datetime.strptime(info['RECORD_TIME_HOUR'], '%Y-%m-%d %H')\n",
    "    hour = (departure - visit_hour).seconds / (24 * 60 * 60) + (departure - visit_hour).days\n",
    "    df_hour_idx.at[idx, 'LOS'] = float(hour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hour['LOS'] = df_hour['LOS_HOUR']\n",
    "df_hour.drop(columns=['LOS_HOUR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_hour\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['LOS'] = df['LOS'].clip(lower=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = df.loc[0].index\n",
    "\n",
    "csv = dict()\n",
    "for key in ['PatientID', 'RecordTime', 'AdmissionTime', 'DischargeTime', 'Outcome', 'LOS', 'Sex', 'Age']:\n",
    "    csv[key] = []\n",
    "for key in index[8:-2]:\n",
    "    csv[key] = []\n",
    "    \n",
    "for pat in tqdm(patient_ids): # for all patients\n",
    "    # get visits for pat.id == PATIENT_ID\n",
    "    info = df[df['PATIENT_ID'] == pat]\n",
    "    info = info[max(0, len(info) - 76):]\n",
    "    idxs = info.index\n",
    "    for i in idxs:\n",
    "        visit = info.loc[i]\n",
    "        for key in index[8:-2]:\n",
    "            csv[key].append(visit[key])\n",
    "        # ['PatientID', 'RecordTime', 'AdmissionTime', 'DischargeTime', 'Outcome', 'LOS', 'Sex', 'Age']\n",
    "        csv['PatientID'].append(visit['PATIENT_ID'])\n",
    "        t, h = visit['RECORD_TIME_HOUR'].split()\n",
    "        t = t.split('-')\n",
    "        csv['RecordTime'].append(t[1]+'/'+t[2]+'/'+t[0]+' '+h) # 2020-04-06 10 -> 04/06/2020 10\n",
    "        t = visit['ADMISSION_DATE'][:10].split('-')\n",
    "        csv['AdmissionTime'].append(t[1]+'/'+t[2]+'/'+t[0])\n",
    "        t = visit['DEPARTURE_DATE'][:10].split('-')\n",
    "        csv['DischargeTime'].append(t[1]+'/'+t[2]+'/'+t[0])\n",
    "        csv['Outcome'].append(visit['OUTCOME'])\n",
    "        csv['LOS'].append(visit['LOS_HOUR'])\n",
    "        csv['Sex'].append(visit['SEX'])\n",
    "        csv['Age'].append(visit['AGE'])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export data to files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(csv).to_csv(os.path.join(data_dir, 'processed', 'cdsl_dataset_formatted.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(data_dir, 'processed', 'cdsl_dataset_formatted.csv'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Record feature names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_records = ['PatientID', 'RecordTime', 'AdmissionTime', 'DischargeTime']\n",
    "target_features = ['Outcome', 'LOS']\n",
    "demographic_features = ['Sex', 'Age']\n",
    "labtest_features = ['MAX_BLOOD_PRESSURE', 'MIN_BLOOD_PRESSURE', 'TEMPERATURE', 'HEART_RATE', 'OXYGEN_SATURATION', 'ADW -- Coeficiente de anisocitosis', 'ADW -- SISTEMATICO DE SANGRE', 'ALB -- ALBUMINA', 'AMI -- AMILASA', 'AP -- ACTIVIDAD DE PROTROMBINA', 'APTT -- TIEMPO DE CEFALINA (APTT)', 'AU -- ACIDO URICO', 'BAS -- Bas¢filos', 'BAS -- SISTEMATICO DE SANGRE', 'BAS% -- Bas¢filos %', 'BAS% -- SISTEMATICO DE SANGRE', 'BD -- BILIRRUBINA DIRECTA', 'BE(b) -- BE(b)', 'BE(b)V -- BE (b)', 'BEecf -- BEecf', 'BEecfV -- BEecf', 'BT -- BILIRRUBINA TOTAL', 'BT -- BILIRRUBINA TOTAL                                                               ', 'CA -- CALCIO                                                                          ', 'CA++ -- Ca++ Gasometria', 'CHCM -- Conc. Hemoglobina Corpuscular Media', 'CHCM -- SISTEMATICO DE SANGRE', 'CK -- CK (CREATINQUINASA)', 'CL -- CLORO', 'CREA -- CREATININA', 'DD -- DIMERO D', 'EOS -- Eosin¢filos', 'EOS -- SISTEMATICO DE SANGRE', 'EOS% -- Eosin¢filos %', 'EOS% -- SISTEMATICO DE SANGRE', 'FA -- FOSFATASA ALCALINA', 'FER -- FERRITINA', 'FIB -- FIBRINàGENO', 'FOS -- FOSFORO', 'G-CORONAV (RT-PCR) -- Tipo de muestra: Exudado Far¡ngeo/Nasofar¡ngeo', 'GGT -- GGT (GAMMA GLUTAMIL TRANSPEPTIDASA)', 'GLU -- GLUCOSA', 'GOT -- GOT (AST)', 'GPT -- GPT (ALT)', 'HCM -- Hemoglobina Corpuscular Media', 'HCM -- SISTEMATICO DE SANGRE', 'HCO3 -- HCO3-', 'HCO3V -- HCO3-', 'HCTO -- Hematocrito', 'HCTO -- SISTEMATICO DE SANGRE', 'HEM -- Hemat¡es', 'HEM -- SISTEMATICO DE SANGRE', 'HGB -- Hemoglobina', 'HGB -- SISTEMATICO DE SANGRE', 'INR -- INR', 'K -- POTASIO', 'LAC -- LACTATO', 'LDH -- LDH', 'LEUC -- Leucocitos', 'LEUC -- SISTEMATICO DE SANGRE', 'LIN -- Linfocitos', 'LIN -- SISTEMATICO DE SANGRE', 'LIN% -- Linfocitos %', 'LIN% -- SISTEMATICO DE SANGRE', 'MG -- MAGNESIO', 'MONO -- Monocitos', 'MONO -- SISTEMATICO DE SANGRE', 'MONO% -- Monocitos %', 'MONO% -- SISTEMATICO DE SANGRE', 'NA -- SODIO', 'NEU -- Neutr¢filos', 'NEU -- SISTEMATICO DE SANGRE', 'NEU% -- Neutr¢filos %', 'NEU% -- SISTEMATICO DE SANGRE', 'PCO2 -- pCO2', 'PCO2V -- pCO2', 'PCR -- PROTEINA C REACTIVA', 'PH -- pH', 'PHV -- pH', 'PLAQ -- Recuento de plaquetas', 'PLAQ -- SISTEMATICO DE SANGRE', 'PO2 -- pO2', 'PO2V -- pO2', 'PROCAL -- PROCALCITONINA', 'PT -- PROTEINAS TOTALES', 'SO2C -- sO2c (Saturaci¢n de ox¡geno)', 'SO2CV -- sO2c (Saturaci¢n de ox¡geno)', 'TCO2 -- tCO2(B)c', 'TCO2V -- tCO2 (B)', 'TP -- TIEMPO DE PROTROMBINA', 'TROPO -- TROPONINA', 'U -- UREA', 'VCM -- SISTEMATICO DE SANGRE', 'VCM -- Volumen Corpuscular Medio', 'VPM -- SISTEMATICO DE SANGRE', 'VPM -- Volumen plaquetar medio', 'VSG -- VSG']\n",
    "\n",
    "require_impute_features = labtest_features\n",
    "normalize_features = ['Age'] + labtest_features + ['LOS']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stratified split dataset into train, validation and test sets\n",
    "\n",
    "- Also include (Imputation & Normalization & Outlier Filtering) steps\n",
    "- The train, validation and test sets are saved in the `./processed` folder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10-fold dataset setting\n",
    "\n",
    "- use 8:1:1 10-fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_folds = 10\n",
    "\n",
    "# Group the dataframe by patient ID\n",
    "grouped = df.groupby('PatientID')\n",
    "\n",
    "# Split the patient IDs into train/val/test sets\n",
    "patients = np.array(list(grouped.groups.keys()))\n",
    "kf = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=SEED)\n",
    "\n",
    "for fold, (train_val_index, test_index) in enumerate(kf.split(patients, df.groupby('PatientID')['Outcome'].first())):\n",
    "    # Get the train/val/test patient IDs for the current fold\n",
    "    train_val_patients, test_patients = patients[train_val_index], patients[test_index]\n",
    "\n",
    "    # Split the train_val_patients into train/val sets\n",
    "    train_patients, val_patients = train_test_split(train_val_patients, test_size=1/(num_folds-1), random_state=SEED, stratify=df[df['PatientID'].isin(train_val_patients)].groupby('PatientID')['Outcome'].first())\n",
    "\n",
    "    # Create train, val, and test dataframes for the current fold\n",
    "    train_df = df[df['PatientID'].isin(train_patients)]\n",
    "    val_df = df[df['PatientID'].isin(val_patients)]\n",
    "    test_df = df[df['PatientID'].isin(test_patients)]\n",
    "    \n",
    "    assert len(train_df) + len(val_df) + len(test_df) == len(df)\n",
    "\n",
    "    # Save the train, val, and test dataframes for the current fold to csv files\n",
    "    \n",
    "    fold_dir = os.path.join(data_dir, 'processed', f'fold_{fold}')\n",
    "    Path(fold_dir).mkdir(parents=True, exist_ok=True)\n",
    "    # train_df.to_csv(os.path.join(fold_dir, \"train_raw.csv\"), index=False)\n",
    "    # val_df.to_csv(os.path.join(fold_dir, \"val_raw.csv\"), index=False)\n",
    "    # test_df.to_csv(os.path.join(fold_dir, \"test_raw.csv\"), index=False)\n",
    "\n",
    "    # Calculate the mean and std of the train set (include age, lab test features, and LOS) on the data in 5% to 95% quantile range\n",
    "\n",
    "    # Normalize data\n",
    "    train_df, val_df, test_df, default_fill, los_info, train_mean, train_std = normalize_dataframe(train_df, val_df, test_df, normalize_features)\n",
    "    \n",
    "    # Drop rows if all features are recorded NaN\n",
    "    train_df = train_df.dropna(axis=0, how='all', subset=normalize_features)\n",
    "    val_df = val_df.dropna(axis=0, how='all', subset=normalize_features)\n",
    "    test_df = test_df.dropna(axis=0, how='all', subset=normalize_features)\n",
    "\n",
    "    # # Save the train, val, and test dataframes for the current fold to csv files\n",
    "    # train_df.to_csv(os.path.join(fold_dir, \"train_after_zscore.csv\"), index=False)\n",
    "    # val_df.to_csv(os.path.join(fold_dir, \"val_after_zscore.csv\"), index=False)\n",
    "    # test_df.to_csv(os.path.join(fold_dir, \"test_after_zscore.csv\"), index=False)\n",
    "\n",
    "    # Forward Imputation after grouped by PatientID\n",
    "    # Notice: if a patient has never done certain lab test, the imputed value will be the median value calculated from train set\n",
    "    train_x, train_y, train_pid = forward_fill_pipeline(train_df, default_fill, demographic_features, labtest_features, target_features, require_impute_features)\n",
    "    val_x, val_y, val_pid = forward_fill_pipeline(val_df, default_fill, demographic_features, labtest_features, target_features, require_impute_features)\n",
    "    test_x, test_y, test_pid = forward_fill_pipeline(test_df, default_fill, demographic_features, labtest_features, target_features, require_impute_features)\n",
    "\n",
    "    # Save the imputed dataset to pickle file\n",
    "    pd.to_pickle(train_x, os.path.join(fold_dir, \"train_x.pkl\"))\n",
    "    pd.to_pickle(train_y, os.path.join(fold_dir, \"train_y.pkl\"))\n",
    "    pd.to_pickle(train_pid, os.path.join(fold_dir, \"train_pid.pkl\"))\n",
    "    pd.to_pickle(val_x, os.path.join(fold_dir, \"val_x.pkl\"))\n",
    "    pd.to_pickle(val_y, os.path.join(fold_dir, \"val_y.pkl\"))\n",
    "    pd.to_pickle(val_pid, os.path.join(fold_dir, \"val_pid.pkl\"))\n",
    "    pd.to_pickle(test_x, os.path.join(fold_dir, \"test_x.pkl\"))\n",
    "    pd.to_pickle(test_y, os.path.join(fold_dir, \"test_y.pkl\"))\n",
    "    pd.to_pickle(test_pid, os.path.join(fold_dir, \"test_pid.pkl\"))\n",
    "    pd.to_pickle(los_info, os.path.join(fold_dir, \"los_info.pkl\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hold-out dataset setting (Stratified)\n",
    "\n",
    "- For CDSL dataset, use 7:1:2 splitting strategy, 70% training, 10% validation, 20% testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the dataframe by patient ID\n",
    "grouped = df.groupby('PatientID')\n",
    "\n",
    "# Get the patient IDs and outcomes\n",
    "patients = np.array(list(grouped.groups.keys()))\n",
    "patients_outcome = np.array([grouped.get_group(patient_id)['Outcome'].iloc[0] for patient_id in patients])\n",
    "\n",
    "# Get the train_val/test patient IDs\n",
    "train_val_patients, test_patients = train_test_split(patients, test_size=20/100, random_state=SEED, stratify=patients_outcome)\n",
    "\n",
    "# Get the train/val patient IDs\n",
    "train_val_patients_outcome = np.array([grouped.get_group(patient_id)['Outcome'].iloc[0] for patient_id in train_val_patients])\n",
    "train_patients, val_patients = train_test_split(train_val_patients, test_size=10/80, random_state=SEED, stratify=train_val_patients_outcome)\n",
    "# Create train, val, test dataframes for the current fold\n",
    "train_df = df[df['PatientID'].isin(train_patients)]\n",
    "val_df = df[df['PatientID'].isin(val_patients)]\n",
    "test_df = df[df['PatientID'].isin(test_patients)]\n",
    "save_dir = os.path.join(data_dir, 'processed', 'fold_10') # forward fill\n",
    "Path(save_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# # Save the train, val, and test dataframes for the current fold to csv files\n",
    "# train_df.to_csv(os.path.join(save_dir, \"train_raw.csv\"), index=False)\n",
    "# val_df.to_csv(os.path.join(save_dir, \"val_raw.csv\"), index=False)\n",
    "# test_df.to_csv(os.path.join(save_dir, \"test_raw.csv\"), index=False)\n",
    "# Calculate the mean and std of the train set (include age, lab test features, and LOS) on the data in 5% to 95% quantile range\n",
    "train_df, val_df, test_df, default_fill, los_info, train_mean, train_std = normalize_dataframe(train_df, val_df, test_df, normalize_features)\n",
    "\n",
    "# # Save the zscored dataframes to csv files\n",
    "# train_df.to_csv(os.path.join(save_dir, \"train_after_zscore.csv\"), index=False)\n",
    "# val_df.to_csv(os.path.join(save_dir, \"val_after_zscore.csv\"), index=False)\n",
    "# test_df.to_csv(os.path.join(save_dir, \"test_after_zscore.csv\"), index=False)\n",
    "\n",
    "# Forward Imputation after grouped by PatientID\n",
    "# Notice: if a patient has never done certain lab test, the imputed value will be the median value calculated from train set\n",
    "train_x, train_y, train_pid = forward_fill_pipeline(train_df, default_fill, demographic_features, labtest_features, target_features, require_impute_features)\n",
    "val_x, val_y, val_pid = forward_fill_pipeline(val_df, default_fill, demographic_features, labtest_features, target_features, require_impute_features)\n",
    "test_x, test_y, test_pid = forward_fill_pipeline(test_df, default_fill, demographic_features, labtest_features, target_features, require_impute_features)\n",
    "\n",
    "# Save the imputed dataset to pickle file\n",
    "pd.to_pickle(train_x, os.path.join(save_dir, \"train_x.pkl\"))\n",
    "pd.to_pickle(train_y, os.path.join(save_dir, \"train_y.pkl\"))\n",
    "pd.to_pickle(train_pid, os.path.join(save_dir, \"train_pid.pkl\"))\n",
    "pd.to_pickle(val_x, os.path.join(save_dir, \"val_x.pkl\"))\n",
    "pd.to_pickle(val_y, os.path.join(save_dir, \"val_y.pkl\"))\n",
    "pd.to_pickle(val_pid, os.path.join(save_dir, \"val_pid.pkl\"))\n",
    "pd.to_pickle(test_x, os.path.join(save_dir, \"test_x.pkl\"))\n",
    "pd.to_pickle(test_y, os.path.join(save_dir, \"test_y.pkl\"))\n",
    "pd.to_pickle(test_pid, os.path.join(save_dir, \"test_pid.pkl\"))\n",
    "pd.to_pickle(los_info, os.path.join(save_dir, \"los_info.pkl\")) # LOS statistics (calculated from the train set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hold-out dataset setting (According to admission time)\n",
    "\n",
    "- For CDSL dataset, use 7:1:2 splitting strategy, 70% training, 10% validation, 20% testing\n",
    "\n",
    "**time order**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the dataframe by PatientID\n",
    "grouped = df.groupby('PatientID')\n",
    "\n",
    "# Get the first AdmissionTime for each patient\n",
    "patients_admission_time = {patient_id: grouped.get_group(patient_id)['AdmissionTime'].iloc[0] for patient_id in grouped.groups.keys()}\n",
    "\n",
    "# Sort patient IDs by first AdmissionTime\n",
    "sorted_patients = sorted(patients_admission_time, key=patients_admission_time.get)\n",
    "\n",
    "# Calculate the indices for splitting\n",
    "train_idx = int(len(sorted_patients) * 0.6)\n",
    "val_idx = int(len(sorted_patients) * 0.8)\n",
    "\n",
    "# Split the patient IDs into train, validation, and test sets\n",
    "train_patients = sorted_patients[:train_idx]\n",
    "val_patients = sorted_patients[train_idx:val_idx]\n",
    "test_patients = sorted_patients[val_idx:]\n",
    "\n",
    "# Create train, val, test, dataframes for the current fold\n",
    "train_df = df[df['PatientID'].isin(train_patients)]\n",
    "val_df = df[df['PatientID'].isin(val_patients)]\n",
    "test_df = df[df['PatientID'].isin(test_patients)]\n",
    "save_dir = os.path.join(data_dir, 'processed', 'fold_11') # forward fill\n",
    "Path(save_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "train_df, val_df, test_df, default_fill, los_info, train_mean, train_std = normalize_dataframe(train_df, val_df, test_df, normalize_features)\n",
    "\n",
    "# Forward Imputation after grouped by PatientID\n",
    "# Notice: if a patient has never done certain lab test, the imputed value will be the median value calculated from train set\n",
    "train_x, train_y, train_pid = forward_fill_pipeline(train_df, default_fill, demographic_features, labtest_features, target_features, require_impute_features)\n",
    "val_x, val_y, val_pid = forward_fill_pipeline(val_df, default_fill, demographic_features, labtest_features, target_features, require_impute_features)\n",
    "test_x, test_y, test_pid = forward_fill_pipeline(test_df, default_fill, demographic_features, labtest_features, target_features, require_impute_features)\n",
    "\n",
    "# Save the imputed dataset to pickle file\n",
    "pd.to_pickle(train_x, os.path.join(save_dir, \"train_x.pkl\"))\n",
    "pd.to_pickle(train_y, os.path.join(save_dir, \"train_y.pkl\"))\n",
    "pd.to_pickle(train_pid, os.path.join(save_dir, \"train_pid.pkl\"))\n",
    "pd.to_pickle(val_x, os.path.join(save_dir, \"val_x.pkl\"))\n",
    "pd.to_pickle(val_y, os.path.join(save_dir, \"val_y.pkl\"))\n",
    "pd.to_pickle(val_pid, os.path.join(save_dir, \"val_pid.pkl\"))\n",
    "pd.to_pickle(test_x, os.path.join(save_dir, \"test_x.pkl\"))\n",
    "pd.to_pickle(test_y, os.path.join(save_dir, \"test_y.pkl\"))\n",
    "pd.to_pickle(test_pid, os.path.join(save_dir, \"test_pid.pkl\"))\n",
    "pd.to_pickle(los_info, os.path.join(save_dir, \"los_info.pkl\")) # LOS statistics (calculated from the train set)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.11 ('python37')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "a10b846bdc9fc41ee38835cbc29d70b69dd5fd54e1341ea2c410a7804a50447a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
