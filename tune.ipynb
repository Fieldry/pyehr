{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch, torch.nn as nn, torch.nn.functional as F, torch.utils.data as data\n",
    "import lightning as L\n",
    "from lightning.pytorch.loggers import CSVLogger\n",
    "from lightning.pytorch.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import optuna\n",
    "import toml\n",
    "\n",
    "import models\n",
    "from datasets.loader.datamodule import EhrDataModule\n",
    "from datasets.loader.load_los_info import get_los_info\n",
    "from datasets.loader.unpad import unpad_y\n",
    "from losses import get_simple_loss\n",
    "from metrics import get_all_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pipeline(L.LightningModule):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.save_hyperparameters()\n",
    "        self.hidden_dim = config[\"hidden_dim\"]\n",
    "        self.input_dim = config[\"input_dim\"]\n",
    "        self.output_dim = config[\"output_dim\"]\n",
    "        model_class = getattr(models, config['model_name'])\n",
    "        self.ehr_encoder = model_class(**config)\n",
    "        if config[\"task\"] == \"outcome\":\n",
    "            self.head = nn.Sequential(nn.Linear(self.hidden_dim, self.output_dim), nn.Dropout(0.0), nn.Sigmoid())\n",
    "        elif config[\"task\"] == \"los\":\n",
    "            self.head = nn.Sequential(nn.Linear(self.hidden_dim, self.output_dim), nn.Dropout(0.0))\n",
    "        elif config[\"task\"] == \"multitask\":\n",
    "            self.head = models.heads.MultitaskHead(self.hidden_dim, self.output_dim, drop=0.0)\n",
    "\n",
    "        self.validation_step_outputs = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedding = self.ehr_encoder(x)\n",
    "        y_hat = self.head(embedding)\n",
    "        return y_hat, embedding\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y, lens, pid = batch\n",
    "        y_hat, embedding = self(x)\n",
    "        y_hat, y = unpad_y(y_hat, y, lens)\n",
    "        loss = get_simple_loss(y_hat, y, self.config[\"task\"])\n",
    "        self.log(\"train_loss_step\", loss, on_step=True, on_epoch=True)\n",
    "        return loss\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y, lens, pid = batch\n",
    "        y_hat, embedding = self(x)\n",
    "        y_hat, y = unpad_y(y_hat, y, lens)\n",
    "        loss = get_simple_loss(y_hat, y, self.config[\"task\"])\n",
    "        self.log(\"val_loss_step\", loss, on_step=True, on_epoch=True)\n",
    "        outs = {'y_pred': y_hat, 'y_true': y, 'val_loss_step': loss}\n",
    "        self.validation_step_outputs.append(outs)\n",
    "        return loss\n",
    "    def on_validation_epoch_end(self):\n",
    "        y_pred = torch.cat([x['y_pred'] for x in self.validation_step_outputs])\n",
    "        y_true = torch.cat([x['y_true'] for x in self.validation_step_outputs])\n",
    "        loss = torch.stack([x['val_loss_step'] for x in self.validation_step_outputs]).mean()\n",
    "        self.log(\"val_loss_epoch\", loss, on_step=False, on_epoch=True)\n",
    "        metrics = get_all_metrics(y_pred, y_true, self.config[\"task\"], self.config[\"los_info\"])\n",
    "        for k, v in metrics.items(): self.log(k, v, on_step=False, on_epoch=True)\n",
    "        main_metric = metrics[self.config[\"main_metric\"]]\n",
    "        self.validation_step_outputs.clear()\n",
    "        return main_metric\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "        return optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"GRU\"\n",
    "stage = \"tune\"\n",
    "dataset = \"tjh\"\n",
    "task = \"outcome\" # [\"outcome\", \"los\", \"multitask\"]\n",
    "fold = 0\n",
    "tjh_config = {\"demo_dim\": 2, \"lab_dim\": 73, \"input_dim\": 75,}\n",
    "cdsl_config = {\"demo_dim\": 2, \"lab_dim\": 97, \"input_dim\": 99,}\n",
    "dataset_config = {}\n",
    "if dataset == \"tjh\": dataset_config = tjh_config\n",
    "elif dataset == \"cdsl\": dataset_config = cdsl_config\n",
    "output_dim = 1\n",
    "main_metric = \"mae\" if task == \"los\" else \"auroc\"\n",
    "epochs = 100\n",
    "patience = 10\n",
    "\n",
    "config = {\"stage\": stage, \"task\": task, \"dataset\": dataset, \"output_dim\": output_dim, \"fold\": fold, \"epochs\": epochs, \"patience\": patience, \"model_name\": model_name, \"main_metric\": main_metric}\n",
    "config = config | dataset_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "- tune: hyperparameter search (Only the first fold)\n",
    "- train: train model with the best hyperparameters (K-fold / repeat with random seeds)\n",
    "- test: test model on the test set with the saved checkpoints (on best epoch)\n",
    "\"\"\"\n",
    "\n",
    "def objective(trial: optuna.trial.Trial):\n",
    "    global config\n",
    "    # config\n",
    "    trial_config = {\n",
    "        \"hidden_dim\": trial.suggest_int(\"hidden_dim\", 16, 1024),\n",
    "        \"batch_size\": trial.suggest_int(\"batch_size\", 1, 16),\n",
    "    }\n",
    "    config = config | trial_config\n",
    "    los_config = get_los_info(f'datasets/{config[\"dataset\"]}/processed/fold_{config[\"fold\"]}')\n",
    "    config[\"los_info\"] = los_config\n",
    "\n",
    "    # data\n",
    "    dm = EhrDataModule(f'datasets/{config[\"dataset\"]}/processed/fold_{config[\"fold\"]}', batch_size=config[\"batch_size\"])\n",
    "    \n",
    "    # callbacks\n",
    "    checkpoint_filename = f'{config[\"model_name\"]}-fold{config[\"fold\"]}'\n",
    "    if config[\"task\"] in [\"outcome\", \"multitask\"]:\n",
    "        early_stopping_callback = EarlyStopping(monitor=\"auprc\", patience=config[\"patience\"], mode=\"max\",)\n",
    "        checkpoint_callback = ModelCheckpoint(save_top_k=1, monitor=\"auprc\", mode=\"max\", dirpath=f'./checkpoints/{config[\"stage\"]}/{config[\"dataset\"]}/{config[\"task\"]}', filename=checkpoint_filename,)\n",
    "    elif config[\"task\"] == \"los\":\n",
    "        early_stopping_callback = EarlyStopping(monitor=\"mae\", patience=config[\"patience\"], mode=\"min\",)\n",
    "        checkpoint_callback = ModelCheckpoint(save_top_k=1, monitor=\"mae\", mode=\"min\", dirpath=f'./checkpoints/{config[\"stage\"]}/{config[\"dataset\"]}/{config[\"task\"]}', filename=checkpoint_filename,)\n",
    "    \n",
    "    # logger\n",
    "    logger = CSVLogger(save_dir=\"logs\", name=f'{config[\"stage\"]}/{config[\"dataset\"]}/{config[\"task\"]}', version=checkpoint_filename, flush_logs_every_n_steps=4)\n",
    "    \n",
    "    # train/val/test\n",
    "    pipeline = Pipeline(config)\n",
    "    trainer = L.Trainer(max_epochs=config[\"epochs\"], logger=logger, callbacks=[early_stopping_callback, checkpoint_callback])\n",
    "    trainer.fit(pipeline, dm)\n",
    "\n",
    "    # return best metric score\n",
    "    best_metric_score = checkpoint_callback.best_model_score\n",
    "    return best_metric_score\n",
    "\n",
    "direction = \"minimize\" if config[\"task\"] == \"los\" else \"maximize\"\n",
    "search_space = {\"hidden_dim\": [64,128], \"batch_size\": [32, 64]}\n",
    "study = optuna.create_study(direction=direction, sampler=optuna.samplers.GridSampler(search_space))\n",
    "study.optimize(objective, n_trials=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial = study.best_trial\n",
    "config = config | trial.params\n",
    "\n",
    "# save the config dict to toml file, with name of {model}-{task}-{score}.toml\n",
    "with open(f'./checkpoints/{config[\"stage\"]}/{config[\"dataset\"]}/{config[\"task\"]}/{config[\"model_name\"]}_best.toml', 'w') as f:\n",
    "    toml.dump(config, f)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
